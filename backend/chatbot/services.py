import os
import logging
from django.conf import settings
from .models import Conversation, Message
from .rag_service import RAGService
from .conversation_handlers import conversation_manager

from langdetect import detect, LangDetectException

logger = logging.getLogger(__name__)

# global instances
_GLOBAL_RAG_SERVICE = None
_GLOBAL_LLM = None


def detect_language(text: str) -> str:

    try:
        detected_lang = detect(text)

        lang_mapping = {
            'fr': 'fr',
            'en': 'en',
            'ar': 'ar',
            'de': 'en',
            'es': 'en',
            'it': 'en',
        }

        language = lang_mapping.get(detected_lang, 'fr')  # Default to French
        logger.info(f" Detected language: {detected_lang} â†’ {language}")
        return language

    except LangDetectException:
        logger.warning("Could not detect language, defaulting to French")
        return 'fr'


class LanguagePrompts:

    PROMPTS = {
        'fr': """Tu es un assistant juridique spÃ©cialisÃ© en droit tunisien des associations.

âš ï¸ RÃˆGLES ABSOLUES (NON-NÃ‰GOCIABLES):

1. RÃ‰PONDS TOUJOURS EN FRANÃ‡AIS - Ta rÃ©ponse DOIT Ãªtre entiÃ¨rement en franÃ§ais
2. UTILISE UNIQUEMENT LE CONTEXTE FOURNI - Aucune autre source d'information
3. SI L'INFO N'EST PAS DANS LE CONTEXTE â†’ RÃ©ponds: "Cette information n'est pas disponible dans les documents que je possÃ¨de."
4. INTERDICTION TOTALE d'inventer:
   - Dates
   - Noms de ministÃ¨res
   - Articles de loi
   - ProcÃ©dures
   - Tout autre dÃ©tail
5. CITE EXACTEMENT le texte du contexte (copie-colle des phrases pertinentes)
6. SI TU AS UN DOUTE â†’ Dis que tu ne sais pas

Exemple de BONNE rÃ©ponse: "Selon le document, [citation exacte du contexte]..."
Exemple de MAUVAISE rÃ©ponse: Inventer des dates ou procÃ©dures non mentionnÃ©es.""",

        'en': """You are a legal assistant specialized in Tunisian association law.

ABSOLUTE RULES (NON-NEGOTIABLE):

1. ALWAYS RESPOND IN FRENCH - Your entire response MUST be in French
2. USE ONLY THE PROVIDED CONTEXT - No other information sources
3. IF INFO IS NOT IN CONTEXT â†’ Respond: "This information is not available in the documents I have."
4. TOTAL PROHIBITION on inventing:
   - Dates
   - Ministry names
   - Law articles
   - Procedures
   - Any other details
5. QUOTE EXACTLY from the context (copy-paste relevant phrases)
6. IF IN DOUBT â†’ Say you don't know

Example of GOOD response: "According to the document, [exact quote from context]..."
Example of BAD response: Making up dates or procedures not mentioned.""",

        'ar': """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ØªÙˆÙ†Ø³ÙŠ Ù„Ù„Ø¬Ù…Ø¹ÙŠØ§Øª.

âš ï¸ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø·Ù„Ù‚Ø© (ØºÙŠØ± Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙØ§ÙˆØ¶):

1. Ø±Ø¯ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø±Ø¯Ùƒ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
2. Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… - Ù„Ø§ Ù…ØµØ§Ø¯Ø± Ø£Ø®Ø±Ù‰
3. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ â†’ Ø±Ø¯: "Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªÙŠ Ø£Ù…Ù„ÙƒÙ‡Ø§."
4. Ø­Ø¸Ø± ØªØ§Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø®ØªØ±Ø§Ø¹:
   - Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
   - Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙˆØ²Ø§Ø±Ø§Øª
   - Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†
   - Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª
   - Ø£ÙŠ ØªÙØ§ØµÙŠÙ„ Ø£Ø®Ø±Ù‰
5. Ø§Ù‚ØªØ¨Ø³ Ø¨Ø§Ù„Ø¶Ø¨Ø· Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ (Ø§Ù†Ø³Ø® Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©)
6. ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø´Ùƒ â†’ Ù‚Ù„ Ø£Ù†Ùƒ Ù„Ø§ ØªØ¹Ø±Ù

Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø±Ø¯ Ø¬ÙŠØ¯: "ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…Ø³ØªÙ†Ø¯ØŒ [Ø§Ù‚ØªØ¨Ø§Ø³ Ø¯Ù‚ÙŠÙ‚ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚]..."
Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø±Ø¯ Ø³ÙŠØ¡: Ø§Ø®ØªØ±Ø§Ø¹ ØªÙˆØ§Ø±ÙŠØ® Ø£Ùˆ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØºÙŠØ± Ù…Ø°ÙƒÙˆØ±Ø©."""
    }

    @classmethod
    def get_prompt(cls, language: str) -> str:
        return cls.PROMPTS.get(language, cls.PROMPTS['fr'])


def get_rag_service():
    global _GLOBAL_RAG_SERVICE
    if _GLOBAL_RAG_SERVICE is None:
        _GLOBAL_RAG_SERVICE = RAGService()
        logger.info(" RAG Service initialized")
    return _GLOBAL_RAG_SERVICE


def get_llm():
    global _GLOBAL_LLM
    if _GLOBAL_LLM is None:
        try:
            from llama_cpp import Llama

            model_path = os.path.join(settings.MEDIA_ROOT, 'models', 'mistral-7b-instruct-v0.1.Q4_K_M.gguf')

            if not os.path.exists(model_path):
                logger.warning(f"Model file not found at {model_path}")
                return None

            logger.info(f"Loading LLM from {model_path}...")
            _GLOBAL_LLM = Llama(
                model_path=model_path,
                n_ctx=4096,
                n_gpu_layers=-1,
                n_batch=512,
                verbose=False
            )
            logger.info("LLM loaded successfully")
        except Exception as e:
            logger.warning(f"LLM not available: {e}")
            logger.warning("You can still ingest documents. LLM will be needed for chat later.")
            return None

    return _GLOBAL_LLM


class ChatbotService:

    def __init__(self):
        self.rag_service = get_rag_service()
        self.llm = get_llm()

    def process_message(self, conversation: Conversation, user_message: str) -> dict:
        try:
            # ğŸŒ DETECT LANGUAGE
            user_language = detect_language(user_message)
            logger.info(f"User language: {user_language.upper()}")

            Message.objects.create(
                conversation=conversation,
                role='user',
                content=user_message
            )

            history = list(
                conversation.messages
                .values('role', 'content')
                .order_by('-created_at')[:5]
            )
            history.reverse()

            relevant_chunks = self.rag_service.retrieve_relevant_chunks(
                query=user_message,
                top_k=3
            )

            logger.info("=" * 80)
            logger.info(f" USER QUERY ({user_language.upper()}): {user_message}")
            logger.info(f" RETRIEVED {len(relevant_chunks)} CHUNKS:")
            for i, chunk in enumerate(relevant_chunks):
                logger.info(f"\n--- Chunk {i + 1} (Relevance Score: {chunk['score']:.4f}) ---")
                logger.info(f"Source: {chunk['title']}")
                logger.info(f"Text Preview: {chunk['text'][:400]}...")
            logger.info("=" * 80)

            if relevant_chunks:
                context = "\n\n".join([
                    f"[Document: {chunk['title']}]\n{chunk['text']}"
                    for chunk in relevant_chunks
                ])
            else:
                no_context_msg = {
                    'fr': "Aucun document pertinent trouvÃ©.",
                    'en': "No relevant documents found.",
                    'ar': "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©."
                }
                context = no_context_msg.get(user_language, no_context_msg['fr'])
                logger.warning(f"No relevant chunks retrieved!")

            history_text = "\n".join([
                f"{msg['role'].upper()}: {msg['content']}"
                for msg in history[:-1]
            ])

            if self.llm is None:
                error_msg = {
                    'fr': "DÃ©solÃ©, le modÃ¨le n'est pas disponible.",
                    'en': "Sorry, the model is not available.",
                    'ar': "Ø¹Ø°Ø±Ø§ØŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ØªØ§Ø­."
                }
                response_text = error_msg.get(user_language, error_msg['fr'])
            else:
                response_text = self._generate_response(
                    user_message=user_message,
                    context=context,
                    history=history_text,
                    language=user_language
                )

            # Log the generated response
            logger.info(f"GENERATED RESPONSE ({user_language.upper()}): {response_text[:200]}...")

            # Save assistant response
            Message.objects.create(
                conversation=conversation,
                role='assistant',
                content=response_text
            )

            return {
                "response": response_text,
                "success": True,
                "sources": [chunk['title'] for chunk in relevant_chunks],
                "language": user_language
            }

        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)
            # Fallback error message in detected language (if possible)
            try:
                error_lang = detect_language(user_message)
            except:
                error_lang = 'fr'

            error_msg = {
                'fr': "DÃ©solÃ©, une erreur s'est produite.",
                'en': "Sorry, an error occurred.",
                'ar': "Ø¹Ø°Ø±Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£."
            }

            return {
                "response": error_msg.get(error_lang, error_msg['fr']),
                "success": False,
                "error": str(e)
            }

    def _generate_response(self, user_message: str, context: str, history: str, language: str = 'fr') -> str:

        system_prompt = LanguagePrompts.get_prompt(language)

        instructions_dict = {
            'fr': {
                'context_label': 'CONTEXTE JURIDIQUE (SEULE SOURCE AUTORISÃ‰E)',
                'history_label': 'Historique de la conversation',
                'question_label': 'QUESTION DE L\'UTILISATEUR',
                'instructions': '- RÃ©ponds en te basant UNIQUEMENT sur le contexte ci-dessus\n- Cite les passages exacts quand c\'est possible\n- Si l\'information n\'existe pas dans le contexte, dis-le clairement\n- Reste factuel et prÃ©cis',
                'response_label': 'RÃ©ponse (basÃ©e strictement sur le contexte)',
            },
            'en': {
                'context_label': 'LEGAL CONTEXT (ONLY AUTHORIZED SOURCE)',
                'history_label': 'Conversation History',
                'question_label': 'USER QUESTION',
                'instructions': '- Answer based ONLY on the context above\n- Quote exact passages when possible\n- If the information doesn\'t exist in the context, state it clearly\n- Stay factual and precise',
                'response_label': 'Response (based strictly on context)',
            },
            'ar': {
                'context_label': 'Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ (Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ù…ØµØ±Ø­ Ø¨Ù‡)',
                'history_label': 'Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©',
                'question_label': 'Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…',
                'instructions': '- Ø±Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ø¹Ù„Ø§Ù‡ ÙÙ‚Ø·\n- Ø§Ù‚ØªØ¨Ø³ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù…ÙƒØ§Ù†\n- Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ø§Ø°ÙƒØ±Ù‡Ø§ Ø¨ÙˆØ¶ÙˆØ­\n- Ø§Ø¨Ù‚ ÙˆØ§Ù‚Ø¹ÙŠØ§Ù‹ ÙˆØ¯Ù‚ÙŠÙ‚Ø§Ù‹',
                'response_label': 'Ø§Ù„Ø±Ø¯ (Ø¨Ù†Ø§Ø¡Ù‹ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚)',
            }
        }

        instr = instructions_dict.get(language, instructions_dict['fr'])

        prompt = f"""<s>[INST] <<SYS>>
{system_prompt}
<</SYS>>

{instr['context_label']}:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{context}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{f"{instr['history_label']}:{history}" if history.strip() else ""}

{instr['question_label']}: {user_message}

{instr['instructions']}

[/INST]

{instr['response_label']}:"""

        logger.info(f"Generating response in: {language.upper()}")
        logger.info(f"System prompt language: {language}")

        # generate with VERY conservative parameters to prevent hallucination
        response = self.llm(
            prompt,
            max_tokens=512,
            temperature=0.05,
            top_p=0.85,
            top_k=20,
            repeat_penalty=1.15,
            frequency_penalty=0.1,
            presence_penalty=0.1,
            stop=["</s>", "[INST]", "Question:", "QUESTION:", "CONTEXTE:", "<s>", "[/INST]"]
        )

        generated_text = response['choices'][0]['text'].strip()

        if any(label in generated_text for label in [instr['response_label'], "RÃ©ponse", "Response", "Ø§Ù„Ø±Ø¯"]):
            for label in [instr['response_label'], "RÃ©ponse", "Response", "Ø§Ù„Ø±Ø¯"]:
                if label in generated_text:
                    generated_text = generated_text.split(label, 1)[-1]
                    break

        generated_text = generated_text.lstrip(': ')

        logger.info(f"Response language: {language}")
        return generated_text.strip()